## **Particle Filter- FastSLAM**
![project][image0]
---

[//]: # (Image References)
[image0]: ./images/viewer.png "result"


# **Overview**
The work of this project shows the step of implementing Particle Filter for SLAM. The work is based on the offline data and visualisation package is taken from the tutorial given by Prof. Claus Brenner. In addition to the original experiment with the feature base approach of landmark association, the project is including an ongoing work on a map base data association without particular landmark matchin ('pf_slam_mapmatching.py'). 


# **FASTSLAM Summary**

**STEP 1- Generate the particles:**  

In this step, we need to generate the new particles using the following robot motion model

_Differential Drive Motion Model (g)_

```sh
	
		l, r := left, right control input (encoder ticks * meter_per_tick)
		w = wheel base distance (width)
		r != l:
		    alpha = (r - l) / w
		    rad = l/alpha
		    x' = x + (rad + w/2.)*(sin(theta+alpha) - sin(theta))
		    y' = y + (rad + w/2.)*(-cos(theta+alpha) + cos(theta))
		    theta' = (theta + alpha + pi) % (2*pi) - pi
		r == l
		    x' = x + l * cos(theta)
		    y' = y + l * sin(theta)
		    theta' = theta

```
With Gaussian distribution of the control motions, the new particles are generated by using the above eq given the control l,r and the current pose.

```sh
		l = random.gauss(left, left_std)
		r = random.gauss(right, right_std)

```



**STEP 2- Compute the weight of all particles:**  
We then compute the weight for each particle based on the actual measurements and the map of this particle based on the following equations.

_compute_correspondence_likelihoods (weight)_  

for each PARTICLE  
```sh
	for each MEASUREMENT 

		1- compute corresponding likelihood with all landmarks within this particle 

			l =         exp(-0.5*zQ)
                              --------------------------
                            (2*pi*sqrt(np.linalg.det(Ql)))
			
		where
			Q_lmk = landmark covariance
			Qt = measurement covariance = [var_range,      0      ] 
   						      [ 0       ,  var_bearing]
			Ql = H*Q_lmk*HT  + Qt
			zQ = (z-z_pred)T * (z-z_pred)
                              ---------------------
 					Ql

		2- find the best measurement-landmark match (maximum l value)

			w, landmark_number = max((v,i) for i,v in enumerate(likelihoods))

		3- There are 2 cases: Init the new landmark or Update the old landmark 
                      
		    **case 1**: New landmark considered from the following condition
				first run, landmark is empty or
 		      	        maximum likelihood is below the threshold 
				
				1- add lmk_x, lmk_y position to this particle map
				   lmk_x = landmark x position in world coordinate
				   lmk_y = landmark y position in world coordinate

				
				2- add landmark covariance to this particle map
				   H_inv = 1/H(X)
				   Qt = measurement covariance
				   Q_lmk = H_inv * Qt * H_inv.T

		    **case 2**: Update the existing landmark

				1- Updated new lmk position  = current lmk position + K(z-z_pred)

				2- Update new lmk covariance = (I-K*H) * current covariance
				
				   where

				      Ql =   H*Q_lmk*HT  + Qt

				      K  =        Q_lmk * HT
					     -------------------
						      Ql

                    
                             
```	

_Measurement function (h)_  

```sh
		z_pred = {r, alpha} = h(X) given by
		dx = lmk_x - (x + scanner_displacement * cos(theta))
        	dy = lmk_y - (y + scanner_displacement * sin(theta))
        	r = sqrt(dx * dx + dy * dy)
        	alpha = (atan2(dy, dx) - state[2] + pi) % (2*pi) - pi

```


_Measurement Jacobian Matrix (H)_  

```sh
		w.r.t the landmark i position (mx, my) 
		[dr_dmx,     dr_dmy     ]
         	[dalpha_dmx, dalpha_dmy ]

		cost, sint = cos(theta), sin(theta)
		dx = landmark[0] - (state[0] + scanner_displacement * cost)
		dy = landmark[1] - (state[1] + scanner_displacement * sint)
		q = dx * dx + dy * dy
		sqrtq = sqrt(q)
		dr_dmx = dx / sqrtq
		dr_dmy = dy / sqrtq
		dalpha_dmx = -dy / q
		dalpha_dmy =  dx / q

		return np.array([[dr_dmx, dr_dmy], [dalpha_dmx, dalpha_dmy]])

```


**STEP 3- Particle Resampling:**  

Resampling wheel method 
```sh
		new_particles = []
		max_weight = max(weights)
		index = random.randint(0, len(self.particles) - 1)
		offset = 0.0
		for i in range(len(self.particles)):
			offset += random.uniform(0, 2.0 * max_weight)
			while offset > weights[index]:
				offset -= weights[index]
				index = (index + 1) % len(weights)
				
			new_particles.append(copy.deepcopy(self.particles[index]))

		return new_particles
```


# **Runing project**
Need python 3.x to run

To run the simulation, in src folder
```sh
$ python pf_slam.py
```

To view result, in src/lib folder
```sh
$ logfile_viewer.py (and select load load fast_slam_correction.txt)
```
